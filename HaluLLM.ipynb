{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dpendeencies"
      ],
      "metadata": {
        "id": "p_GGUaRfBYkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€ MASTER SETUP CELL (always run this first) â”€â”€\n",
        "!apt-get install -y zstd -q\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import subprocess, time\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(5)\n",
        "\n",
        "!ollama pull mistral\n",
        "!ollama pull llama3.2\n",
        "!ollama pull qwen2.5\n",
        "!ollama pull deepseek-r1\n",
        "\n",
        "!pip install ollama pandas tabulate -q\n",
        "\n",
        "print(\"âœ… Everything is ready!\")"
      ],
      "metadata": {
        "id": "RLS9HxZ8A2AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "jseLqd91Bfh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# â”€â”€ Your models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "MODELS = [\n",
        "    \"mistral\",\n",
        "    \"llama3.2\",\n",
        "    \"qwen2.5\",\n",
        "    \"deepseek-r1\",    # â† add this\n",
        "]\n",
        "\n",
        "# â”€â”€ Prompt 1: The actual question â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "PROMPT_1 = \"who won the nobel price for mathametics in 2007\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# ROUND 1: Get all model responses to Prompt 1\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"=\" * 60)\n",
        "print(\"ROUND 1: All models answering the prompt\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "round1_results = {}\n",
        "\n",
        "for model in MODELS:\n",
        "    print(f\"\\n--- {model} responding ---\")\n",
        "    try:\n",
        "        start = time.time()\n",
        "        response = ollama.chat(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": PROMPT_1}\n",
        "            ],\n",
        "            options={\"temperature\": 0.7}\n",
        "        )\n",
        "        elapsed = round(time.time() - start, 2)\n",
        "        output = response['message']['content']\n",
        "        round1_results[model] = output\n",
        "        print(f\"âœ… Done in {elapsed}s\")\n",
        "        print(output)\n",
        "\n",
        "    except Exception as e:\n",
        "        round1_results[model] = f\"ERROR: {e}\"\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# BUILD PROMPT 2: Bundle all Round 1 outputs\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "bundled_responses = \"\"\n",
        "for model, response in round1_results.items():\n",
        "    bundled_responses += f\"\\n\\n### Response by {model}:\\n{response}\"\n",
        "\n",
        "PROMPT_2 = f\"\"\"You are a strict and impartial AI evaluation judge.\n",
        "\n",
        "The following question was posed to multiple AI models:\n",
        "QUESTION: \"{PROMPT_1}\"\n",
        "\n",
        "Here are their responses:\n",
        "{bundled_responses}\n",
        "\n",
        "---\n",
        "Evaluate each response on the following criteria:\n",
        "\n",
        "1. FACTUAL ACCURACY â€” Are the facts stated correct?\n",
        "2. HALLUCINATION â€” Did the model make up or fabricate anything?\n",
        "3. COMPLETENESS â€” Did it fully answer the question?\n",
        "4. REASONING QUALITY â€” Was the logic sound and well structured?\n",
        "\n",
        "For each model provide:\n",
        "- A hallucination score out of 10 (0 = none, 10 = severe)\n",
        "- A quality score out of 10 (0 = poor, 10 = excellent)\n",
        "- One specific example of what was wrong or hallucinated (if any)\n",
        "- A final verdict: TRUSTWORTHY / PARTIALLY TRUSTWORTHY / NOT TRUSTWORTHY\n",
        "\n",
        "End with a final summary stating which model gave the most reliable answer\n",
        "and which gave the most hallucinated answer.\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# ROUND 2: Each model evaluates all Round 1 outputs\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\n\\n\" + \"=\" * 60)\n",
        "print(\"ROUND 2: Each model judging all responses for hallucination\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "round2_results = {}\n",
        "\n",
        "for model in MODELS:\n",
        "    print(f\"\\n--- {model} evaluating ---\")\n",
        "    try:\n",
        "        start = time.time()\n",
        "        response = ollama.chat(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a strict and impartial AI evaluation judge. Be objective, precise and structured in your evaluation.\"},\n",
        "                {\"role\": \"user\", \"content\": PROMPT_2}\n",
        "            ],\n",
        "            options={\"temperature\": 0.3}\n",
        "        )\n",
        "        elapsed = round(time.time() - start, 2)\n",
        "        evaluation = response['message']['content']\n",
        "        round2_results[model] = evaluation\n",
        "        print(f\"âœ… Evaluation done in {elapsed}s\")\n",
        "        print(evaluation)\n",
        "\n",
        "    except Exception as e:\n",
        "        round2_results[model] = f\"ERROR: {e}\"\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ET00zhyTA_ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just for the visual representation"
      ],
      "metadata": {
        "id": "jRJOYsNyBQhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# â”€â”€ Step 1: Extract scores from Round 2 evaluations â”€â”€\n",
        "def extract_scores(evaluation_text):\n",
        "    \"\"\"Pull hallucination and quality scores from evaluation text\"\"\"\n",
        "    hallucination_scores = {}\n",
        "    quality_scores = {}\n",
        "\n",
        "    # Find all score patterns like \"8/10\" or \"8 out of 10\"\n",
        "    lines = evaluation_text.split('\\n')\n",
        "    current_model = None\n",
        "\n",
        "    for line in lines:\n",
        "        # Detect which model is being discussed\n",
        "        for model_name in round1_results.keys():\n",
        "            if model_name.lower() in line.lower():\n",
        "                current_model = model_name\n",
        "\n",
        "        if current_model:\n",
        "            # Extract hallucination score\n",
        "            hall_match = re.search(r'hallucination[^\\d]*(\\d+)\\s*(?:/|out of)\\s*10', line, re.IGNORECASE)\n",
        "            if hall_match:\n",
        "                hallucination_scores[current_model] = int(hall_match.group(1))\n",
        "\n",
        "            # Extract quality score\n",
        "            qual_match = re.search(r'quality[^\\d]*(\\d+)\\s*(?:/|out of)\\s*10', line, re.IGNORECASE)\n",
        "            if qual_match:\n",
        "                quality_scores[current_model] = int(qual_match.group(1))\n",
        "\n",
        "    return hallucination_scores, quality_scores\n",
        "\n",
        "# â”€â”€ Step 2: Aggregate scores across all judges â”€â”€\n",
        "all_hallucination = {model: [] for model in round1_results.keys()}\n",
        "all_quality = {model: [] for model in round1_results.keys()}\n",
        "\n",
        "for judge, evaluation in round2_results.items():\n",
        "    h_scores, q_scores = extract_scores(evaluation)\n",
        "\n",
        "    for model, score in h_scores.items():\n",
        "        all_hallucination[model].append(score)\n",
        "    for model, score in q_scores.items():\n",
        "        all_quality[model].append(score)\n",
        "\n",
        "# â”€â”€ Step 3: Build leaderboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "leaderboard = []\n",
        "for model in round1_results.keys():\n",
        "    h_scores = all_hallucination[model]\n",
        "    q_scores = all_quality[model]\n",
        "\n",
        "    avg_hallucination = round(sum(h_scores) / len(h_scores), 2) if h_scores else \"N/A\"\n",
        "    avg_quality = round(sum(q_scores) / len(q_scores), 2) if q_scores else \"N/A\"\n",
        "\n",
        "    leaderboard.append({\n",
        "        \"Model\": model,\n",
        "        \"Avg Hallucination Score (lower=better)\": avg_hallucination,\n",
        "        \"Avg Quality Score (higher=better)\": avg_quality,\n",
        "        \"Judges Count\": len(h_scores)\n",
        "    })\n",
        "\n",
        "# Sort by hallucination score (lower is better)\n",
        "df_leaderboard = pd.DataFrame(leaderboard)\n",
        "df_leaderboard = df_leaderboard.sort_values(\"Avg Hallucination Score (lower=better)\")\n",
        "\n",
        "print(\"\\nğŸ† FINAL LEADERBOARD\\n\")\n",
        "display(df_leaderboard)"
      ],
      "metadata": {
        "id": "BU46Wu7sA_ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Work yet to do:"
      ],
      "metadata": {
        "id": "1KyF9TdcBusI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Inference model : for evaluation metrics\n",
        "2.   Code refinement : for AI Detection\n",
        "3.   Visualization for the metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pZlTrzoCBudz"
      }
    }
  ]
}